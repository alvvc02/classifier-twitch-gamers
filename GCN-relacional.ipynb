{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la clase del modelo GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(input_dim, hidden_dim)\n",
    "        self.gc2 = GraphConvolution(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la clase para la capa de convolución de grafos\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        ##print(x.shape)       para comprobar las dimensiones de x \n",
    "        ##                      y ver que se puede mult con adj\n",
    "        x = torch.matmul(adj, x)\n",
    "        x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     views  mature  life_time  created_at  updated_at  numeric_id  \\\n",
      "0     7879       1        969  2016-02-16  2018-10-12           0   \n",
      "1      500       0       2699  2011-05-19  2018-10-08           1   \n",
      "2   382502       1       3149  2010-02-27  2018-10-12           2   \n",
      "3      386       0       1344  2015-01-26  2018-10-01           3   \n",
      "4     2486       0       1784  2013-11-22  2018-10-11           4   \n",
      "..     ...     ...        ...         ...         ...         ...   \n",
      "95    1108       1       1163  2015-08-05  2018-10-11          95   \n",
      "96     257       0        775  2016-08-19  2018-10-03          96   \n",
      "97   47789       1       3234  2009-12-04  2018-10-12          97   \n",
      "98    2154       1       2075  2013-02-01  2018-10-08          98   \n",
      "99    2060       1       3272  2009-10-04  2018-09-19          99   \n",
      "\n",
      "    dead_account language  affiliate  \n",
      "0              0       EN          1  \n",
      "1              0       EN          0  \n",
      "2              0       EN          1  \n",
      "3              0       EN          0  \n",
      "4              0       EN          0  \n",
      "..           ...      ...        ...  \n",
      "95             0       EN          0  \n",
      "96             0       EN          0  \n",
      "97             0       EN          1  \n",
      "98             0       EN          0  \n",
      "99             0       EN          0  \n",
      "\n",
      "[100 rows x 9 columns]\n",
      "    numeric_id_1  numeric_id_2\n",
      "0          98343        141493\n",
      "1          98343         58736\n",
      "2          98343        140703\n",
      "3          98343        151401\n",
      "4          98343        157118\n",
      "..           ...           ...\n",
      "94        141493        155216\n",
      "95        141493        131128\n",
      "96        141493         34043\n",
      "97        141493         52154\n",
      "98        141493         34817\n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de vértices y aristas desde los archivos CSV\n",
    "vertices_data = pd.read_csv('trabajo raiz/large_twitch_features.csv')[:100]\n",
    "aristas_data = pd.read_csv('trabajo raiz/large_twitch_edges.csv')[:99]\n",
    "print(vertices_data)\n",
    "print(aristas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 100 nodes and 99 edges\n"
     ]
    }
   ],
   "source": [
    "# Construir el grafo a partir de los datos de vértices y aristas\n",
    "graph = nx.from_pandas_edgelist(aristas_data, 'numeric_id_1', 'numeric_id_2', create_using=nx.Graph())\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Obtener las características y etiquetas de los vértices\n",
    "labels = vertices_data['affiliate'].values\n",
    "print(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.879e+03 1.000e+00 9.690e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##Codificacion de 'created_at', 'updated_at' y 'language'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Seleccionar las columnas de características categóricas\n",
    "categorical_columns = ['created_at', 'updated_at', 'language']\n",
    "\n",
    "# Codificar las columnas categóricas utilizando one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_features = encoder.fit_transform(vertices_data[categorical_columns])\n",
    "\n",
    "# Obtener las columnas de características numéricas\n",
    "numeric_columns = ['views', 'mature', 'life_time', 'numeric_id', 'dead_account']\n",
    "\n",
    "# Combinar las características numéricas y codificadas en un solo array\n",
    "features = np.concatenate([vertices_data[numeric_columns].values, encoded_features], axis=1)\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "train_indices, test_indices = train_test_split(range(len(features)), test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "# Preparar los datos en tensores de PyTorch\n",
    "features = torch.FloatTensor(features)\n",
    "labels = torch.LongTensor(labels)\n",
    "adj = nx.adjacency_matrix(graph)\n",
    "print(adj.shape)\n",
    "adj = torch.FloatTensor(adj.todense())\n",
    "\n",
    "train_features = features[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "train_adj = adj[train_indices, :][:, train_indices]\n",
    "\n",
    "val_features = features[val_indices]\n",
    "val_labels = labels[val_indices]\n",
    "val_adj = adj[val_indices, :][:, val_indices]\n",
    "\n",
    "test_features = features[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "test_adj = adj[test_indices, :][:, test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 133])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Definir los parámetros del modelo y del entrenamiento\n",
    "input_dim = features.shape[1]\n",
    "print(features.shape)\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(labels))\n",
    "print(output_dim)\n",
    "lr = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo GCN\n",
    "model = GCN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento\n",
    "def train(model, features, adj, labels, train_indices, epochs, lr):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj)\n",
    "        loss = F.cross_entropy(output[train_indices], labels[train_indices])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch: {:04d} | Loss: {:.4f}'.format(epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 | Loss: nan\n",
      "Epoch: 0002 | Loss: nan\n",
      "Epoch: 0003 | Loss: nan\n",
      "Epoch: 0004 | Loss: nan\n",
      "Epoch: 0005 | Loss: nan\n",
      "Epoch: 0006 | Loss: nan\n",
      "Epoch: 0007 | Loss: nan\n",
      "Epoch: 0008 | Loss: nan\n",
      "Epoch: 0009 | Loss: nan\n",
      "Epoch: 0010 | Loss: nan\n",
      "Epoch: 0011 | Loss: nan\n",
      "Epoch: 0012 | Loss: nan\n",
      "Epoch: 0013 | Loss: nan\n",
      "Epoch: 0014 | Loss: nan\n",
      "Epoch: 0015 | Loss: nan\n",
      "Epoch: 0016 | Loss: nan\n",
      "Epoch: 0017 | Loss: nan\n",
      "Epoch: 0018 | Loss: nan\n",
      "Epoch: 0019 | Loss: nan\n",
      "Epoch: 0020 | Loss: nan\n",
      "Epoch: 0021 | Loss: nan\n",
      "Epoch: 0022 | Loss: nan\n",
      "Epoch: 0023 | Loss: nan\n",
      "Epoch: 0024 | Loss: nan\n",
      "Epoch: 0025 | Loss: nan\n",
      "Epoch: 0026 | Loss: nan\n",
      "Epoch: 0027 | Loss: nan\n",
      "Epoch: 0028 | Loss: nan\n",
      "Epoch: 0029 | Loss: nan\n",
      "Epoch: 0030 | Loss: nan\n",
      "Epoch: 0031 | Loss: nan\n",
      "Epoch: 0032 | Loss: nan\n",
      "Epoch: 0033 | Loss: nan\n",
      "Epoch: 0034 | Loss: nan\n",
      "Epoch: 0035 | Loss: nan\n",
      "Epoch: 0036 | Loss: nan\n",
      "Epoch: 0037 | Loss: nan\n",
      "Epoch: 0038 | Loss: nan\n",
      "Epoch: 0039 | Loss: nan\n",
      "Epoch: 0040 | Loss: nan\n",
      "Epoch: 0041 | Loss: nan\n",
      "Epoch: 0042 | Loss: nan\n",
      "Epoch: 0043 | Loss: nan\n",
      "Epoch: 0044 | Loss: nan\n",
      "Epoch: 0045 | Loss: nan\n",
      "Epoch: 0046 | Loss: nan\n",
      "Epoch: 0047 | Loss: nan\n",
      "Epoch: 0048 | Loss: nan\n",
      "Epoch: 0049 | Loss: nan\n",
      "Epoch: 0050 | Loss: nan\n",
      "Epoch: 0051 | Loss: nan\n",
      "Epoch: 0052 | Loss: nan\n",
      "Epoch: 0053 | Loss: nan\n",
      "Epoch: 0054 | Loss: nan\n",
      "Epoch: 0055 | Loss: nan\n",
      "Epoch: 0056 | Loss: nan\n",
      "Epoch: 0057 | Loss: nan\n",
      "Epoch: 0058 | Loss: nan\n",
      "Epoch: 0059 | Loss: nan\n",
      "Epoch: 0060 | Loss: nan\n",
      "Epoch: 0061 | Loss: nan\n",
      "Epoch: 0062 | Loss: nan\n",
      "Epoch: 0063 | Loss: nan\n",
      "Epoch: 0064 | Loss: nan\n",
      "Epoch: 0065 | Loss: nan\n",
      "Epoch: 0066 | Loss: nan\n",
      "Epoch: 0067 | Loss: nan\n",
      "Epoch: 0068 | Loss: nan\n",
      "Epoch: 0069 | Loss: nan\n",
      "Epoch: 0070 | Loss: nan\n",
      "Epoch: 0071 | Loss: nan\n",
      "Epoch: 0072 | Loss: nan\n",
      "Epoch: 0073 | Loss: nan\n",
      "Epoch: 0074 | Loss: nan\n",
      "Epoch: 0075 | Loss: nan\n",
      "Epoch: 0076 | Loss: nan\n",
      "Epoch: 0077 | Loss: nan\n",
      "Epoch: 0078 | Loss: nan\n",
      "Epoch: 0079 | Loss: nan\n",
      "Epoch: 0080 | Loss: nan\n",
      "Epoch: 0081 | Loss: nan\n",
      "Epoch: 0082 | Loss: nan\n",
      "Epoch: 0083 | Loss: nan\n",
      "Epoch: 0084 | Loss: nan\n",
      "Epoch: 0085 | Loss: nan\n",
      "Epoch: 0086 | Loss: nan\n",
      "Epoch: 0087 | Loss: nan\n",
      "Epoch: 0088 | Loss: nan\n",
      "Epoch: 0089 | Loss: nan\n",
      "Epoch: 0090 | Loss: nan\n",
      "Epoch: 0091 | Loss: nan\n",
      "Epoch: 0092 | Loss: nan\n",
      "Epoch: 0093 | Loss: nan\n",
      "Epoch: 0094 | Loss: nan\n",
      "Epoch: 0095 | Loss: nan\n",
      "Epoch: 0096 | Loss: nan\n",
      "Epoch: 0097 | Loss: nan\n",
      "Epoch: 0098 | Loss: nan\n",
      "Epoch: 0099 | Loss: nan\n",
      "Epoch: 0100 | Loss: nan\n"
     ]
    }
   ],
   "source": [
    "train(model, features, adj, labels, train_indices, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Función de predicción\n",
    "def predict(model, features, adj, test_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        predictions = output[test_indices].argmax(dim=1)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.8790e+03, 1.0000e+00, 9.6900e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [5.0000e+02, 0.0000e+00, 2.6990e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [3.8250e+05, 1.0000e+00, 3.1490e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [4.7789e+04, 1.0000e+00, 3.2340e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.1540e+03, 1.0000e+00, 2.0750e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0600e+03, 1.0000e+00, 3.2720e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "torch.Size([100, 100])\n",
      "[83, 53, 70, 45, 44, 39, 22, 80, 10, 0, 18, 30, 73, 33, 90, 4, 76, 77, 12, 31]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features)\n",
    "print(adj.shape)\n",
    "print(test_indices)\n",
    "predict(model, features, adj, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.55\n"
     ]
    }
   ],
   "source": [
    "## Evaluar el modelo\n",
    "def evaluate_model(model, features, adj, labels, test_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        predictions = output[test_indices].argmax(dim=1)\n",
    "        accuracy = torch.sum(predictions == labels[test_indices]).item() / len(test_indices)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "accuracy = evaluate_model(model, features, adj, labels, test_indices)\n",
    "print(f\"Accuracy on test set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
